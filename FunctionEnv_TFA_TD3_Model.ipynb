{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FunctionEnv-TFA-TD3-Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAwXsv5USYvv"
      },
      "source": [
        "Instalando o tf_agents caso não esteja instalado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whz4yRiYjPw6"
      },
      "source": [
        "!pip3 install -q tf-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMx-60e-Smrd"
      },
      "source": [
        "Funções para serem utilizadas durante o teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeihCn-NjYci"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "# === Multi-modal ===\r\n",
        "# D dimensões - Múltiplos mínimos\r\n",
        "def ackley(x: np.ndarray, a=20, b=0.2, c=2 * np.math.pi):\r\n",
        "    d = x.shape[0]\r\n",
        "    return -a * np.exp(-b * np.sqrt(np.sum(x * x, axis=0) / d)) - \\\r\n",
        "           np.exp(np.sum(np.cos(c * x), axis=0) / d) + a + np.math.e\r\n",
        "\r\n",
        "\r\n",
        "# D dimensões - Múltiplos mínimos\r\n",
        "def griewank(x: np.ndarray):\r\n",
        "    sum = np.sum(x ** 2, axis=0) / 4000.0\r\n",
        "    den = np.arange(start=1, stop=(x.shape[0] + 1), dtype=x.dtype)\r\n",
        "    prod = np.cos(x / np.sqrt(den))\r\n",
        "    prod = np.prod(prod, axis=0)\r\n",
        "    return sum - prod + 1\r\n",
        "\r\n",
        "\r\n",
        "# D dimensões - Múltiplos mínimos\r\n",
        "def rastrigin(x: np.ndarray):\r\n",
        "    d = x.shape[0]\r\n",
        "    return 10 * d + np.sum(x ** 2 - 10 * np.cos(x * 2 * np.math.pi), axis=0)\r\n",
        "\r\n",
        "\r\n",
        "def levy(x: np.ndarray):\r\n",
        "    pi = np.math.pi\r\n",
        "    d = x.shape[0] - 1\r\n",
        "    w = 1 + (x - 1) / 4\r\n",
        "\r\n",
        "    term1 = np.sin(pi * w[0]) ** 2\r\n",
        "    term3 = (w[d] - 1) ** 2 * (1 + np.sin(2 * pi * w[d]) ** 2)\r\n",
        "\r\n",
        "    wi = w[0:d]\r\n",
        "    sum = np.sum((wi - 1) ** 2 * (1 + 10 * np.sin(pi * wi + 1) ** 2), axis=0)\r\n",
        "    return term1 + sum + term3\r\n",
        "\r\n",
        "\r\n",
        "# === Valley-shaped ===\r\n",
        "# D dimensões\r\n",
        "def rosenbrock(x: np.ndarray):\r\n",
        "    rosen_sum = 0.0\r\n",
        "    d = x.shape[0]\r\n",
        "\r\n",
        "    for i in range(d - 1):\r\n",
        "        rosen_sum += 100 * (x[i + 1] - x[i] ** 2) ** 2 + (x[i] - 1.0) ** 2\r\n",
        "\r\n",
        "    return rosen_sum\r\n",
        "\r\n",
        "\r\n",
        "# === Plate-shaped ===\r\n",
        "# D dimensões\r\n",
        "def zakharov(x: np.ndarray):\r\n",
        "    d = x.shape[0]\r\n",
        "\r\n",
        "    sum1 = np.sum(x * x, axis=0)\r\n",
        "    sum2 = np.sum(x * np.arange(start=1, stop=(d + 1), dtype=x.dtype) / 2, axis=0)\r\n",
        "    return sum1 + sum2 ** 2 + sum2 ** 4\r\n",
        "\r\n",
        "\r\n",
        "# === Bowl-shaped ===\r\n",
        "# 2 Dimensões\r\n",
        "def bohachevsky(x: np.ndarray):\r\n",
        "    d = x.shape[0]\r\n",
        "    assert d == 2\r\n",
        "\r\n",
        "    return x[0] ** 2 + 2 * (x[1] ** 2) - 0.3 * np.cos(3 * np.pi * x[0]) - 0.4 * np.cos(4 * np.pi * x[1]) + 0.7\r\n",
        "\r\n",
        "\r\n",
        "# D dimensões\r\n",
        "def sum_squares(x: np.ndarray):\r\n",
        "    mul = np.arange(start=1, stop=(x.shape[0] + 1), dtype=x.dtype)\r\n",
        "    return np.sum((x ** 2) * mul, axis=0)\r\n",
        "\r\n",
        "\r\n",
        "# D dimensões\r\n",
        "def sphere(x: np.ndarray):\r\n",
        "    return np.sum(x * x, axis=0)\r\n",
        "\r\n",
        "\r\n",
        "# D dimensões:\r\n",
        "def rotated_hyper_ellipsoid(x: np.ndarray):\r\n",
        "    d = x.shape[0]\r\n",
        "\r\n",
        "    return np.sum([np.sum(x[0:(i + 1)] ** 2, axis=0) for i in range(d)], dtype=np.float32, axis=0)\r\n",
        "\r\n",
        "\r\n",
        "# === Funções Utilitárias ===\r\n",
        "# Recebe uma função como argumento\r\n",
        "# Retorna o 'limite inferior e superior' da função\r\n",
        "def get_low_and_high(function):\r\n",
        "    if function is sphere or function is rastrigin:\r\n",
        "        return -5.12, 5.12\r\n",
        "    elif function is ackley:\r\n",
        "        return -32.768, 32.768\r\n",
        "    elif function is rosenbrock or function is zakharov:\r\n",
        "        return -5.0, 10.0\r\n",
        "    elif function is bohachevsky:\r\n",
        "        return -100.0, 100.0\r\n",
        "    elif function is sum_squares or function is levy:\r\n",
        "        return -10.0, 10.0\r\n",
        "    elif function is griewank:\r\n",
        "        return -600.0, 600.0\r\n",
        "    elif function is rotated_hyper_ellipsoid:\r\n",
        "        return -65.536, 65.536\r\n",
        "\r\n",
        "# Bom dia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az4fqfcqSrMT"
      },
      "source": [
        "Definindo o ambiente de otimização de funções como py_environment\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNQxAogEjK2-"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "from tf_agents.environments import py_environment\r\n",
        "from tf_agents.specs import array_spec\r\n",
        "from tf_agents.trajectories import time_step as ts\r\n",
        "from collections import namedtuple\r\n",
        "\r\n",
        "hypercube = namedtuple('hypercube', ['min', 'max'])\r\n",
        "\r\n",
        "\r\n",
        "class FunctionEnv(py_environment.PyEnvironment):\r\n",
        "    def __init__(self, function, domain: hypercube, dims) -> None:\r\n",
        "        super().__init__()\r\n",
        "        self._function = function\r\n",
        "        self._domain = domain\r\n",
        "        self._dims = dims\r\n",
        "        self._best_solution = np.finfo(np.float32).max\r\n",
        "        self._episode_ended = False\r\n",
        "        self._steps_taken = 0\r\n",
        "        self._state = np.random.uniform(size=(dims,), low=domain.min, high=domain.max) \\\r\n",
        "            .astype(dtype=np.float32, copy=False)\r\n",
        "\r\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\r\n",
        "            shape=(dims,), dtype=np.float32,\r\n",
        "            minimum=-1.0, maximum=1.0, name='action')\r\n",
        "\r\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\r\n",
        "            shape=(dims,), dtype=np.float32,\r\n",
        "            minimum=domain.min, maximum=domain.max, name='observation')\r\n",
        "\r\n",
        "    def action_spec(self):\r\n",
        "        return self._action_spec\r\n",
        "\r\n",
        "    def observation_spec(self):\r\n",
        "        return self._observation_spec\r\n",
        "\r\n",
        "    def get_info(self):\r\n",
        "        return self._best_solution\r\n",
        "\r\n",
        "    def get_state(self):\r\n",
        "        state = (self._state, self._steps_taken, self._episode_ended)\r\n",
        "        return state\r\n",
        "\r\n",
        "    def set_state(self, state):\r\n",
        "        _state, _steps_taken, _episode_ended = state\r\n",
        "        self._state = _state\r\n",
        "        self._steps_taken = _steps_taken\r\n",
        "        self._episode_ended = _episode_ended\r\n",
        "\r\n",
        "    def _step(self, action):\r\n",
        "        if self._episode_ended:\r\n",
        "            return self.reset()\r\n",
        "\r\n",
        "        self._state = self._state + 2.5*action\r\n",
        "        self._state = np.clip(self._state, a_min=self._domain.min, a_max=self._domain.max)\r\n",
        "\r\n",
        "        self._steps_taken += 1\r\n",
        "        if self._steps_taken > 2000:\r\n",
        "            self._episode_ended = True\r\n",
        "\r\n",
        "        reward = -self._function(self._state)\r\n",
        "        if reward < self._best_solution:\r\n",
        "            self._best_solution = reward\r\n",
        "\r\n",
        "        if self._episode_ended:\r\n",
        "            return ts.termination(self._state, reward)\r\n",
        "        else:\r\n",
        "            return ts.transition(self._state, reward)\r\n",
        "\r\n",
        "    def _reset(self):\r\n",
        "        self._state = np.random.uniform(size=(self._dims,), low=self._domain.min, high=self._domain.max) \\\r\n",
        "            .astype(dtype=np.float32, copy=False)\r\n",
        "        self._episode_ended = False\r\n",
        "        self._steps_taken = 0\r\n",
        "        return ts.restart(self._state)\r\n",
        "\r\n",
        "    def _render(self):\r\n",
        "        # TODO: Implementar método para renderizar\r\n",
        "        pass\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a60xjf_hS6_5"
      },
      "source": [
        "Função para testar o agente treinado (Plota fitness x iteração)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fVLzmb8uHXZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def evaluate_agent(eval_env, policy_eval, function, dims):\r\n",
        "    time_step = eval_env.reset()\r\n",
        "    \r\n",
        "    pos = time_step.observation.numpy()[0]\r\n",
        "    best_solution = function(pos)\r\n",
        "\r\n",
        "    best_solution_at_it = []\r\n",
        "    best_solution_at_it.append(best_solution)\r\n",
        "    best_it = 0\r\n",
        "    it = 0\r\n",
        "\r\n",
        "    while not time_step.is_last():\r\n",
        "        it += 1\r\n",
        "        action_step = policy_eval.action(time_step)\r\n",
        "        time_step = eval_env.step(action_step.action)\r\n",
        "\r\n",
        "        obj_value = -time_step.reward.numpy()[0]\r\n",
        "\r\n",
        "        if obj_value < best_solution:\r\n",
        "            best_solution = obj_value\r\n",
        "            pos = time_step.observation.numpy()[0]\r\n",
        "            best_it = it\r\n",
        "            \r\n",
        "        best_solution_at_it.append(best_solution)\r\n",
        "          \r\n",
        "    fig, ax = plt.subplots()\r\n",
        "    ax.plot(range(len(best_solution_at_it)), best_solution_at_it)\r\n",
        "    ax.set(xlabel=\"Iteration\", ylabel=\"Best objective value\", title=\"TD3 on {0} ({1} Dims)\".format(function.__name__, dims))\r\n",
        "    ax.grid()\r\n",
        "    plt.show()\r\n",
        "    print('best_solution: ', best_solution)\r\n",
        "    print('found at it: ', best_it)\r\n",
        "    print('at position: ', pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgluRyTFTEsx"
      },
      "source": [
        "Imports para Main (Agente, Redes, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMKXaEfFThlr"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tf_agents.environments.wrappers import TimeLimit\r\n",
        "from tf_agents.environments import tf_py_environment\r\n",
        "from tf_agents.agents import Td3Agent\r\n",
        "from tf_agents.agents.ddpg.actor_network import ActorNetwork\r\n",
        "from tf_agents.agents.ddpg.critic_network import CriticNetwork\r\n",
        "from tf_agents.drivers import dynamic_step_driver\r\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\r\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZWRoyyTtp1"
      },
      "source": [
        "Hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgAUp1A4TtI3"
      },
      "source": [
        "# Hiperparametros de treino\r\n",
        "num_episodes = 800 # @param {type:\"integer\"}\r\n",
        "initial_collect_episodes = 10 # @param {type:\"integer\"}\r\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\r\n",
        "\r\n",
        "# Hiperparametros da memória de replay\r\n",
        "buffer_size = 1000000 # @param {type:\"integer\"}\r\n",
        "batch_size = 64 # @param {type:\"number\"}\r\n",
        "\r\n",
        "# Hiperparametros do Agente\r\n",
        "actor_lr = 1e-4 # @param {type:\"number\"}\r\n",
        "critic_lr = 2e-4 # @param {type:\"number\"}\r\n",
        "tau = 5e-4 # @param {type:\"number\"}\r\n",
        "discount = 0.99 # @param {type:\"number\"}\r\n",
        "exploration_noise_std = 0.2 # @param {type:\"number\"}\r\n",
        "target_policy_noise = 0.2 # @param {type:\"number\"}\r\n",
        "target_policy_noise_clip = 0.5 # @param {type:\"number\"}\r\n",
        "actor_update_period = 2 # @param {type:\"integer\"}\r\n",
        "target_update_period = 2 # @param {type:\"integer\"}\r\n",
        "reward_scale_factor = 0.75 # @param {type:\"number\"}\r\n",
        "\r\n",
        "# --- Arquitetura da rede ---\r\n",
        "# Actor\r\n",
        "fc_layer_params = [400, 300] # FNN's do Actor\r\n",
        "# Critic\r\n",
        "observation_fc_layer_params = [400] # FNN's apenas para observações\r\n",
        "joint_fc_layer_params=[300] # FNN's depois de concatenar (observação, ação)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KSpCFFyTl58"
      },
      "source": [
        "Criando o Env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0cpe9RT9lt"
      },
      "source": [
        "# Envs\r\n",
        "steps = 500  # @param {type:\"integer\"}\r\n",
        "steps_eval = 2000 # @param {type:\"integer\"}\r\n",
        "dims = 2  # @param {type:\"integer\"}\r\n",
        "function = ackley # @param [\"sphere\", \"ackley\", \"griewank\", \"levy\", \"zakharov\", \"rotated_hyper_ellipsoid\", \"rosenbrock\"]{type: \"raw\"}\r\n",
        "low, high = get_low_and_high(function)\r\n",
        "\r\n",
        "env = FunctionEnv(function=function, domain=hypercube(min=low, max=high), dims=dims)\r\n",
        "\r\n",
        "env_training = TimeLimit(env=env, duration=steps)\r\n",
        "env_eval = TimeLimit(env=env, duration=steps_eval)\r\n",
        "tf_env_training = tf_py_environment.TFPyEnvironment(environment=env_training)\r\n",
        "tf_env_eval = tf_py_environment.TFPyEnvironment(environment=env_eval)\r\n",
        "\r\n",
        "obs_spec = tf_env_training.observation_spec()\r\n",
        "act_spec = tf_env_training.action_spec()\r\n",
        "time_spec = tf_env_training.time_step_spec()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSP3PBJwUCD0"
      },
      "source": [
        "Criando as redes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djan2SvxUG5G"
      },
      "source": [
        "# Creating networks\r\n",
        "actor_network = ActorNetwork(input_tensor_spec=obs_spec,\r\n",
        "                             output_tensor_spec=act_spec,\r\n",
        "                             fc_layer_params=fc_layer_params,\r\n",
        "                             activation_fn=tf.keras.activations.relu)\r\n",
        "critic_network = CriticNetwork(input_tensor_spec=(obs_spec, act_spec),\r\n",
        "                               observation_fc_layer_params=observation_fc_layer_params,\r\n",
        "                               joint_fc_layer_params=joint_fc_layer_params,\r\n",
        "                               activation_fn=tf.keras.activations.relu,\r\n",
        "                               output_activation_fn=tf.keras.activations.linear)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVbb91AiUIdI"
      },
      "source": [
        "Criando o agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTaOktR1UPdw"
      },
      "source": [
        "# Creating agent\r\n",
        "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\r\n",
        "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\r\n",
        "\r\n",
        "agent = Td3Agent(\r\n",
        "    time_step_spec=time_spec,\r\n",
        "    action_spec=act_spec,\r\n",
        "    actor_network=actor_network,\r\n",
        "    critic_network=critic_network,\r\n",
        "    actor_optimizer=actor_optimizer,\r\n",
        "    critic_optimizer=critic_optimizer,\r\n",
        "    target_update_tau=tau,\r\n",
        "    exploration_noise_std=exploration_noise_std,\r\n",
        "    target_policy_noise=target_policy_noise,\r\n",
        "    target_policy_noise_clip=target_policy_noise_clip,\r\n",
        "    actor_update_period=actor_update_period,\r\n",
        "    target_update_period=target_update_period,\r\n",
        "    reward_scale_factor=reward_scale_factor,\r\n",
        "    train_step_counter=tf.Variable(0),\r\n",
        "    gamma=discount)\r\n",
        "\r\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW2zDV7cURNi"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNBYocZnUTqe"
      },
      "source": [
        "# Replay buffer\r\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=agent.collect_data_spec,\r\n",
        "                                                               batch_size=tf_env_training.batch_size,\r\n",
        "                                                               max_length=buffer_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVQKdOabUVSy"
      },
      "source": [
        "Criando o Driver e realizando coleta inicial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQVXZMW3UYo7"
      },
      "source": [
        "# Data Collection (Collect for initial episodes)\r\n",
        "driver = dynamic_step_driver.DynamicStepDriver(env=tf_env_training,\r\n",
        "                                               policy=agent.collect_policy,\r\n",
        "                                               observers=[replay_buffer.add_batch],\r\n",
        "                                               num_steps=collect_steps_per_iteration)\r\n",
        "driver.run = common.function(driver.run)\r\n",
        "\r\n",
        "\r\n",
        "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(env=tf_env_training,\r\n",
        "                                               policy=agent.collect_policy,\r\n",
        "                                               observers=[replay_buffer.add_batch],\r\n",
        "                                               num_steps=collect_steps_per_iteration)\r\n",
        "\r\n",
        "initial_collect_driver.run = common.function(initial_collect_driver.run)\r\n",
        "\r\n",
        "for _ in range(initial_collect_episodes):\r\n",
        "    done = False\r\n",
        "    while not done:\r\n",
        "        time_step, _ = initial_collect_driver.run()\r\n",
        "        done = time_step.is_last()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdyE8wpdUdW6"
      },
      "source": [
        "Criando o dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyqQoALeUfye"
      },
      "source": [
        "# Creating a dataset\r\n",
        "dataset = replay_buffer.as_dataset(\r\n",
        "    sample_batch_size=batch_size,\r\n",
        "    num_steps=2)\r\n",
        "\r\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldLYm4-NUfU_"
      },
      "source": [
        "Treinamento do Agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA51bmP_mNVy"
      },
      "source": [
        "# Training\r\n",
        "agent.train = common.function(agent.train)\r\n",
        "agent.train_step_counter.assign(0)\r\n",
        "\r\n",
        "for ep in range(num_episodes):\r\n",
        "    done = False\r\n",
        "    best_solution = tf.float32.max\r\n",
        "    ep_rew = 0.0\r\n",
        "    while not done:\r\n",
        "        time_step, _ = driver.run()\r\n",
        "        experience, unused_info = next(iterator)\r\n",
        "        agent.train(experience)\r\n",
        "\r\n",
        "        obj_value = -time_step.reward.numpy()[0]\r\n",
        "\r\n",
        "        if obj_value < best_solution and not time_step.is_first():\r\n",
        "            best_solution = obj_value\r\n",
        "\r\n",
        "        ep_rew += -obj_value\r\n",
        "        done = time_step.is_last()\r\n",
        "\r\n",
        "    print('episode = {0} Best solution on episode: {1} Return on episode: {2}'.format(ep, best_solution, ep_rew))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqJzanqWUi3_"
      },
      "source": [
        "Realizando os testes do agente depois que sendo chamado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfnjCK1DqgV4"
      },
      "source": [
        "evaluate_agent(tf_env_eval, agent.policy, function, dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTldFX6KJ2nC"
      },
      "source": [
        "evaluate_agent(tf_env_eval, agent.collect_policy, function, dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR-bXvyWcHPT"
      },
      "source": [
        "Salvando ambas policies e agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEDlq5sxcGN6"
      },
      "source": [
        "from tf_agents.policies.policy_saver import PolicySaver\r\n",
        "\r\n",
        "tf_policy_saver = PolicySaver(agent.policy)\r\n",
        "tf_policy_collect_saver = PolicySaver(agent.collect_policy)\r\n",
        "\r\n",
        "tf_policy_saver.save('policy')\r\n",
        "tf_policy_collect_saver.save('policy_collect')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}