{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "FunctionEnv-TFA-TD3-Model.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAwXsv5USYvv"
   },
   "source": [
    "Instalando o tf_agents caso não esteja instalado"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Whz4yRiYjPw6"
   },
   "source": [
    "!pip3 install -q tf-agents"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importando as funções e o ambiente."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jMx-60e-Smrd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from functions.numpy_functions import *\n",
    "from environments.py_function_environment import *"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a60xjf_hS6_5"
   },
   "source": [
    "Função para testar o agente treinado (Plota fitness x iteração)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2fVLzmb8uHXZ"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_agent(eval_env, policy_eval, function, dims):\n",
    "    time_step = eval_env.reset()\n",
    "    \n",
    "    pos = time_step.observation.numpy()[0]\n",
    "    best_solution = function(pos)\n",
    "\n",
    "    best_solution_at_it = []\n",
    "    best_solution_at_it.append(best_solution)\n",
    "    best_it = 0\n",
    "    it = 0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "        it += 1\n",
    "        action_step = policy_eval.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "\n",
    "        obj_value = -time_step.reward.numpy()[0]\n",
    "\n",
    "        if obj_value < best_solution:\n",
    "            best_solution = obj_value\n",
    "            pos = time_step.observation.numpy()[0]\n",
    "            best_it = it\n",
    "            \n",
    "        best_solution_at_it.append(best_solution)\n",
    "          \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(len(best_solution_at_it)), best_solution_at_it)\n",
    "    ax.set(xlabel=\"Iteration\", ylabel=\"Best objective value\", title=\"TD3 on {0} ({1} Dims)\".format(function.__name__, dims))\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "    print('best_solution: ', best_solution)\n",
    "    print('found at it: ', best_it)\n",
    "    print('at position: ', pos)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgluRyTFTEsx"
   },
   "source": [
    "Imports para Main (Agente, Redes, etc)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LMKXaEfFThlr"
   },
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.environments.wrappers import TimeLimit\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.agents import Td3Agent\n",
    "from tf_agents.agents.ddpg.actor_network import ActorNetwork\n",
    "from tf_agents.agents.ddpg.critic_network import CriticNetwork\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hZWRoyyTtp1"
   },
   "source": [
    "Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MgAUp1A4TtI3"
   },
   "source": [
    "# Hiperparametros de treino\n",
    "num_episodes = 800 # @param {type:\"integer\"}\n",
    "initial_collect_episodes = 10 # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "\n",
    "# Hiperparametros da memória de replay\n",
    "buffer_size = 1000000 # @param {type:\"integer\"}\n",
    "batch_size = 64 # @param {type:\"number\"}\n",
    "\n",
    "# Hiperparametros do Agente\n",
    "actor_lr = 1e-4 # @param {type:\"number\"}\n",
    "critic_lr = 2e-4 # @param {type:\"number\"}\n",
    "tau = 5e-4 # @param {type:\"number\"}\n",
    "discount = 0.99 # @param {type:\"number\"}\n",
    "exploration_noise_std = 0.2 # @param {type:\"number\"}\n",
    "target_policy_noise = 0.2 # @param {type:\"number\"}\n",
    "target_policy_noise_clip = 0.5 # @param {type:\"number\"}\n",
    "actor_update_period = 2 # @param {type:\"integer\"}\n",
    "target_update_period = 2 # @param {type:\"integer\"}\n",
    "reward_scale_factor = 0.75 # @param {type:\"number\"}\n",
    "\n",
    "# --- Arquitetura da rede ---\n",
    "# Actor\n",
    "fc_layer_params = [400, 300] # FNN's do Actor\n",
    "# Critic\n",
    "observation_fc_layer_params = [400] # FNN's apenas para observações\n",
    "joint_fc_layer_params=[300] # FNN's depois de concatenar (observação, ação)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KSpCFFyTl58"
   },
   "source": [
    "Criando o Env"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZG0cpe9RT9lt"
   },
   "source": [
    "# Envs\n",
    "steps = 500  # @param {type:\"integer\"}\n",
    "steps_eval = 2000  # @param {type:\"integer\"}\n",
    "dims = 2  # @param {type:\"integer\"}\n",
    "function = Ackley() # @param [\"Sphere()\", \"Ackley()\", \"Griewank()\", \"Levy()\", \"Zakharov()\", \"RotatedHyperEllipsoid()\", \"Rosenbrock()\"]{type: \"raw\"}\n",
    "\n",
    "env = PyFunctionEnvironment(function=function, dims=dims)\n",
    "\n",
    "env_training = TimeLimit(env=env, duration=steps)\n",
    "env_eval = TimeLimit(env=env, duration=steps_eval)\n",
    "tf_env_training = tf_py_environment.TFPyEnvironment(environment=env_training)\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(environment=env_eval)\n",
    "\n",
    "obs_spec = tf_env_training.observation_spec()\n",
    "act_spec = tf_env_training.action_spec()\n",
    "time_spec = tf_env_training.time_step_spec()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSP3PBJwUCD0"
   },
   "source": [
    "Criando as redes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "djan2SvxUG5G"
   },
   "source": [
    "# Creating networks\r\n",
    "actor_network = ActorNetwork(input_tensor_spec=obs_spec,\r\n",
    "                             output_tensor_spec=act_spec,\r\n",
    "                             fc_layer_params=fc_layer_params,\r\n",
    "                             activation_fn=tf.keras.activations.relu)\r\n",
    "critic_network = CriticNetwork(input_tensor_spec=(obs_spec, act_spec),\r\n",
    "                               observation_fc_layer_params=observation_fc_layer_params,\r\n",
    "                               joint_fc_layer_params=joint_fc_layer_params,\r\n",
    "                               activation_fn=tf.keras.activations.relu,\r\n",
    "                               output_activation_fn=tf.keras.activations.linear)\r\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVbb91AiUIdI"
   },
   "source": [
    "Criando o agente"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NTaOktR1UPdw"
   },
   "source": [
    "# Creating agent\r\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\r\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\r\n",
    "\r\n",
    "agent = Td3Agent(\r\n",
    "    time_step_spec=time_spec,\r\n",
    "    action_spec=act_spec,\r\n",
    "    actor_network=actor_network,\r\n",
    "    critic_network=critic_network,\r\n",
    "    actor_optimizer=actor_optimizer,\r\n",
    "    critic_optimizer=critic_optimizer,\r\n",
    "    target_update_tau=tau,\r\n",
    "    exploration_noise_std=exploration_noise_std,\r\n",
    "    target_policy_noise=target_policy_noise,\r\n",
    "    target_policy_noise_clip=target_policy_noise_clip,\r\n",
    "    actor_update_period=actor_update_period,\r\n",
    "    target_update_period=target_update_period,\r\n",
    "    reward_scale_factor=reward_scale_factor,\r\n",
    "    train_step_counter=tf.Variable(0),\r\n",
    "    gamma=discount)\r\n",
    "\r\n",
    "agent.initialize()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW2zDV7cURNi"
   },
   "source": [
    "Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jNBYocZnUTqe"
   },
   "source": [
    "# Replay buffer\r\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=agent.collect_data_spec,\r\n",
    "                                                               batch_size=tf_env_training.batch_size,\r\n",
    "                                                               max_length=buffer_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVQKdOabUVSy"
   },
   "source": [
    "Criando o Driver e realizando coleta inicial"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PQVXZMW3UYo7"
   },
   "source": [
    "# Data Collection (Collect for initial episodes)\r\n",
    "driver = dynamic_step_driver.DynamicStepDriver(env=tf_env_training,\r\n",
    "                                               policy=agent.collect_policy,\r\n",
    "                                               observers=[replay_buffer.add_batch],\r\n",
    "                                               num_steps=collect_steps_per_iteration)\r\n",
    "driver.run = common.function(driver.run)\r\n",
    "\r\n",
    "\r\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(env=tf_env_training,\r\n",
    "                                               policy=agent.collect_policy,\r\n",
    "                                               observers=[replay_buffer.add_batch],\r\n",
    "                                               num_steps=collect_steps_per_iteration)\r\n",
    "\r\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\r\n",
    "\r\n",
    "for _ in range(initial_collect_episodes):\r\n",
    "    done = False\r\n",
    "    while not done:\r\n",
    "        time_step, _ = initial_collect_driver.run()\r\n",
    "        done = time_step.is_last()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdyE8wpdUdW6"
   },
   "source": [
    "Criando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vyqQoALeUfye"
   },
   "source": [
    "# Creating a dataset\r\n",
    "dataset = replay_buffer.as_dataset(\r\n",
    "    sample_batch_size=batch_size,\r\n",
    "    num_steps=2)\r\n",
    "\r\n",
    "iterator = iter(dataset)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldLYm4-NUfU_"
   },
   "source": [
    "Treinamento do Agente"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CA51bmP_mNVy"
   },
   "source": [
    "# Training\r\n",
    "agent.train = common.function(agent.train)\r\n",
    "agent.train_step_counter.assign(0)\r\n",
    "\r\n",
    "for ep in range(num_episodes):\r\n",
    "    done = False\r\n",
    "    best_solution = tf.float32.max\r\n",
    "    ep_rew = 0.0\r\n",
    "    while not done:\r\n",
    "        time_step, _ = driver.run()\r\n",
    "        experience, unused_info = next(iterator)\r\n",
    "        agent.train(experience)\r\n",
    "\r\n",
    "        obj_value = -time_step.reward.numpy()[0]\r\n",
    "\r\n",
    "        if obj_value < best_solution and not time_step.is_first():\r\n",
    "            best_solution = obj_value\r\n",
    "\r\n",
    "        ep_rew += -obj_value\r\n",
    "        done = time_step.is_last()\r\n",
    "\r\n",
    "    print('episode = {0} Best solution on episode: {1} Return on episode: {2}'.format(ep, best_solution, ep_rew))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqJzanqWUi3_"
   },
   "source": [
    "Realizando os testes do agente depois que sendo chamado"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mfnjCK1DqgV4"
   },
   "source": [
    "evaluate_agent(tf_env_eval, agent.policy, function, dims)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wTldFX6KJ2nC"
   },
   "source": [
    "evaluate_agent(tf_env_eval, agent.collect_policy, function, dims)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR-bXvyWcHPT"
   },
   "source": [
    "Salvando ambas policies e agente"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OEDlq5sxcGN6"
   },
   "source": [
    "from tf_agents.policies.policy_saver import PolicySaver\r\n",
    "\r\n",
    "tf_policy_saver = PolicySaver(agent.policy)\r\n",
    "tf_policy_collect_saver = PolicySaver(agent.collect_policy)\r\n",
    "\r\n",
    "tf_policy_saver.save('policy')\r\n",
    "tf_policy_collect_saver.save('policy_collect')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}