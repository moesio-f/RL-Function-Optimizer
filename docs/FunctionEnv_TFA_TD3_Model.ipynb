{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAwXsv5USYvv"
   },
   "source": [
    "Instalando o tf_agents caso não esteja instalado e fazendo uma cópia\n",
    "do github para a sessão atual (Utilizado no Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Whz4yRiYjPw6"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q tf-agents\n",
    "!git clone https://github.com/moesio-f/RL-Function-Optimizer\n",
    "import sys\n",
    "sys.path.append('/content/RL-Function-Optimizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Importando as funções e o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMx-60e-Smrd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functions.numpy_functions import *\n",
    "from environments.py_function_environment import *\n",
    "from utils.evaluation import evaluate_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgluRyTFTEsx"
   },
   "source": [
    "Imports para Main (Agente, Redes, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMKXaEfFThlr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.environments.wrappers import TimeLimit\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.agents import Td3Agent\n",
    "from tf_agents.agents.ddpg.actor_network import ActorNetwork\n",
    "from tf_agents.agents.ddpg.critic_network import CriticNetwork\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hZWRoyyTtp1"
   },
   "source": [
    "Hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgAUp1A4TtI3"
   },
   "outputs": [],
   "source": [
    "# Hiperparametros de treino\n",
    "num_episodes = 800 # @param {type:\"integer\"}\n",
    "initial_collect_episodes = 10 # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "\n",
    "# Hiperparametros da memória de replay\n",
    "buffer_size = 1000000 # @param {type:\"integer\"}\n",
    "batch_size = 64 # @param {type:\"number\"}\n",
    "\n",
    "# Hiperparametros do Agente\n",
    "actor_lr = 1e-4 # @param {type:\"number\"}\n",
    "critic_lr = 2e-4 # @param {type:\"number\"}\n",
    "tau = 5e-4 # @param {type:\"number\"}\n",
    "discount = 0.99 # @param {type:\"number\"}\n",
    "exploration_noise_std = 0.2 # @param {type:\"number\"}\n",
    "target_policy_noise = 0.2 # @param {type:\"number\"}\n",
    "target_policy_noise_clip = 0.5 # @param {type:\"number\"}\n",
    "actor_update_period = 2 # @param {type:\"integer\"}\n",
    "target_update_period = 2 # @param {type:\"integer\"}\n",
    "reward_scale_factor = 0.75 # @param {type:\"number\"}\n",
    "\n",
    "# --- Arquitetura da rede ---\n",
    "# Actor\n",
    "fc_layer_params = [400, 300] # FNN's do Actor\n",
    "# Critic\n",
    "observation_fc_layer_params = [400] # FNN's apenas para observações\n",
    "joint_fc_layer_params=[300] # FNN's depois de concatenar (observação, ação)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KSpCFFyTl58"
   },
   "source": [
    "Criando o Env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG0cpe9RT9lt"
   },
   "outputs": [],
   "source": [
    "# Envs\n",
    "steps = 500  # @param {type:\"integer\"}\n",
    "steps_eval = 2000  # @param {type:\"integer\"}\n",
    "dims = 2  # @param {type:\"integer\"}\n",
    "function = Ackley() # @param [\"Sphere()\", \"Ackley()\", \"Griewank()\", \"Levy()\", \"Zakharov()\", \"RotatedHyperEllipsoid()\", \"Rosenbrock()\"]{type: \"raw\"}\n",
    "\n",
    "env = PyFunctionEnvironment(function=function, dims=dims)\n",
    "\n",
    "env_training = TimeLimit(env=env, duration=steps)\n",
    "env_eval = TimeLimit(env=env, duration=steps_eval)\n",
    "tf_env_training = tf_py_environment.TFPyEnvironment(environment=env_training)\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(environment=env_eval)\n",
    "\n",
    "obs_spec = tf_env_training.observation_spec()\n",
    "act_spec = tf_env_training.action_spec()\n",
    "time_spec = tf_env_training.time_step_spec()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSP3PBJwUCD0"
   },
   "source": [
    "Criando as redes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djan2SvxUG5G"
   },
   "outputs": [],
   "source": [
    "# Creating networks\n",
    "actor_network = ActorNetwork(input_tensor_spec=obs_spec,\n",
    "                             output_tensor_spec=act_spec,\n",
    "                             fc_layer_params=fc_layer_params,\n",
    "                             activation_fn=tf.keras.activations.relu)\n",
    "critic_network = CriticNetwork(input_tensor_spec=(obs_spec, act_spec),\n",
    "                               observation_fc_layer_params=observation_fc_layer_params,\n",
    "                               joint_fc_layer_params=joint_fc_layer_params,\n",
    "                               activation_fn=tf.keras.activations.relu,\n",
    "                               output_activation_fn=tf.keras.activations.linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVbb91AiUIdI"
   },
   "source": [
    "Criando o agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTaOktR1UPdw"
   },
   "outputs": [],
   "source": [
    "# Creating agent\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "\n",
    "agent = Td3Agent(\n",
    "    time_step_spec=time_spec,\n",
    "    action_spec=act_spec,\n",
    "    actor_network=actor_network,\n",
    "    critic_network=critic_network,\n",
    "    actor_optimizer=actor_optimizer,\n",
    "    critic_optimizer=critic_optimizer,\n",
    "    target_update_tau=tau,\n",
    "    exploration_noise_std=exploration_noise_std,\n",
    "    target_policy_noise=target_policy_noise,\n",
    "    target_policy_noise_clip=target_policy_noise_clip,\n",
    "    actor_update_period=actor_update_period,\n",
    "    target_update_period=target_update_period,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    train_step_counter=tf.Variable(0),\n",
    "    gamma=discount)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW2zDV7cURNi"
   },
   "source": [
    "Replay Buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNBYocZnUTqe"
   },
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=agent.collect_data_spec,\n",
    "                                                               batch_size=tf_env_training.batch_size,\n",
    "                                                               max_length=buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVQKdOabUVSy"
   },
   "source": [
    "Criando o Driver e realizando coleta inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQVXZMW3UYo7"
   },
   "outputs": [],
   "source": [
    "# Data Collection (Collect for initial episodes)\n",
    "driver = dynamic_step_driver.DynamicStepDriver(env=tf_env_training,\n",
    "                                               policy=agent.collect_policy,\n",
    "                                               observers=[replay_buffer.add_batch],\n",
    "                                               num_steps=collect_steps_per_iteration)\n",
    "driver.run = common.function(driver.run)\n",
    "\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(env=tf_env_training,\n",
    "                                               policy=agent.collect_policy,\n",
    "                                               observers=[replay_buffer.add_batch],\n",
    "                                               num_steps=collect_steps_per_iteration)\n",
    "\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "\n",
    "for _ in range(initial_collect_episodes):\n",
    "    done = False\n",
    "    while not done:\n",
    "        time_step, _ = initial_collect_driver.run()\n",
    "        done = time_step.is_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdyE8wpdUdW6"
   },
   "source": [
    "Criando o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyqQoALeUfye"
   },
   "outputs": [],
   "source": [
    "# Creating a dataset\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldLYm4-NUfU_"
   },
   "source": [
    "Treinamento do Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CA51bmP_mNVy"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "agent.train = common.function(agent.train)\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    done = False\n",
    "    best_solution = tf.float32.max\n",
    "    ep_rew = 0.0\n",
    "    while not done:\n",
    "        time_step, _ = driver.run()\n",
    "        experience, unused_info = next(iterator)\n",
    "        agent.train(experience)\n",
    "\n",
    "        obj_value = -time_step.reward.numpy()[0]\n",
    "\n",
    "        if obj_value < best_solution and not time_step.is_first():\n",
    "            best_solution = obj_value\n",
    "\n",
    "        ep_rew += -obj_value\n",
    "        done = time_step.is_last()\n",
    "\n",
    "    print('episode = {0} Best solution on episode: {1} Return on episode: {2}'.format(ep, best_solution, ep_rew))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqJzanqWUi3_"
   },
   "source": [
    "Realizando os testes do agente (Policy e Collect Policy) para 1 único episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfnjCK1DqgV4"
   },
   "outputs": [],
   "source": [
    "evaluate_agent(tf_env_eval, agent.policy, function, dims, name_algorithm='TD3', save_to_file=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTldFX6KJ2nC"
   },
   "outputs": [],
   "source": [
    "evaluate_agent(tf_env_eval, agent.collect_policy, function, dims, name_algorithm='TD3', save_to_file=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR-bXvyWcHPT"
   },
   "source": [
    "Salvando ambas policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEDlq5sxcGN6"
   },
   "outputs": [],
   "source": [
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "\n",
    "tf_policy_saver = PolicySaver(agent.policy)\n",
    "tf_policy_collect_saver = PolicySaver(agent.collect_policy)\n",
    "\n",
    "tf_policy_saver.save('policy')\n",
    "tf_policy_collect_saver.save('policy_collect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fazendo download dos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from os import system\n",
    "agent_file_name = f'policies-TD3-{function.name}-{str(dims)}d.zip'\n",
    "system(f'zip -r {agent_file_name} policy/ policy_collect/')\n",
    "files.download(agent_file_name)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FunctionEnv-TFA-TD3-Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}