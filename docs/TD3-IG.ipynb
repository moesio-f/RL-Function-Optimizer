{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAwXsv5USYvv"
   },
   "source": [
    "Instalando o TF Agents 0.7.1 caso não esteja instalado, fazendo uma cópia\n",
    "do github para a sessão atual e adicionando o caminho para os diretórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Whz4yRiYjPw6"
   },
   "outputs": [],
   "source": [
    "!pip3 install 'tf-agents==0.7.1'\n",
    "!git clone https://github.com/moesio-f/RL-Function-Optimizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/RL-Function-Optimizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMx-60e-Smrd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# TF Agents\n",
    "from tf_agents.environments.wrappers import TimeLimit\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.agents.ddpg.critic_network import CriticNetwork\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "\n",
    "# Locais\n",
    "from functions.numpy_functions import *\n",
    "from environments.py_function_environment import *\n",
    "from environments.py_env_wrappers import RewardClip, RewardScale\n",
    "from networks.custom_actor_network import CustomActorNetwork\n",
    "from agents.td3_inverting_gradients import Td3AgentInvertingGradients\n",
    "from utils.evaluation import evaluate_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgluRyTFTEsx"
   },
   "source": [
    "Hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMKXaEfFThlr"
   },
   "outputs": [],
   "source": [
    "num_episodes = 2000  # @param {type:\"integer\"}\n",
    "initial_collect_episodes = 20  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "\n",
    "# Hiperparametros da memória de replay\n",
    "buffer_size = 1000000  # @param {type:\"integer\"}\n",
    "batch_size = 256  # @param {type:\"number\"}\n",
    "\n",
    "# Hiperparametros do Agente\n",
    "actor_lr = 1e-5  # @param {type:\"number\"}\n",
    "critic_lr = 2e-4  # @param {type:\"number\"}\n",
    "tau = 1e-4  # @param {type:\"number\"}\n",
    "actor_update_period = 2  # @param {type:\"integer\"}\n",
    "target_update_period = 2  # @param {type:\"integer\"}\n",
    "\n",
    "discount = 0.99  # @param {type:\"number\"}\n",
    "gradient_clipping_norm = 2.5  # @param {type:\"number\"}\n",
    "\n",
    "exploration_noise_std = 0.5  # @param {type:\"number\"}\n",
    "exploration_noise_std_end = 0.1  # @param {type: \"number\"}\n",
    "exploration_noise_num_episodes = 1850# @param {type: \"number\"}\n",
    "target_policy_noise = 0.2  # @param {type:\"number\"}\n",
    "target_policy_noise_clip = 0.5  # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hZWRoyyTtp1"
   },
   "source": [
    "Definindo a arquitetura das Redes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgAUp1A4TtI3"
   },
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "fc_layer_params = [256, 256]  # FNN's do Actor\n",
    "\n",
    "# Critic Network\n",
    "action_fc_layer_params = None  # FNN's apenas para ações\n",
    "observation_fc_layer_params = None  # FNN's apenas para observações\n",
    "joint_fc_layer_params = [256, 256]  # FNN's depois de concatenar (observação, ação)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KSpCFFyTl58"
   },
   "source": [
    "Criando os ambientes de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG0cpe9RT9lt"
   },
   "outputs": [],
   "source": [
    "steps = 250  # @param {type:\"integer\"}\n",
    "steps_eval = 500  # @param {type:\"integer\"}\n",
    "\n",
    "dims = 2  # @param {type:\"integer\"}\n",
    "function = Sphere()  # @param [\"Rastrigin()\",\"SumSquares()\", \"Sphere()\", \"Ackley()\", \"Griewank()\", \"Levy()\", \"Zakharov()\", \"RotatedHyperEllipsoid()\", \"Rosenbrock()\", \"DixonPrice()\"]{type: \"raw\"}\n",
    "\n",
    "env = PyFunctionEnvironment(function=function, dims=dims, clip_actions=False)\n",
    "\n",
    "env_training = TimeLimit(env=env, duration=steps)\n",
    "env_eval = TimeLimit(env=env, duration=steps_eval)\n",
    "\n",
    "tf_env_training = tf_py_environment.TFPyEnvironment(environment=env_training)\n",
    "tf_env_eval = tf_py_environment.TFPyEnvironment(environment=env_eval)\n",
    "\n",
    "obs_spec = tf_env_training.observation_spec()\n",
    "act_spec = tf_env_training.action_spec()\n",
    "time_spec = tf_env_training.time_step_spec()\n",
    "\n",
    "# Atualizando a exploração para ser determinada com base nos steps\n",
    "exploration_noise_num_steps = round(exploration_noise_num_episodes * steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSP3PBJwUCD0"
   },
   "source": [
    "Criando as redes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djan2SvxUG5G"
   },
   "outputs": [],
   "source": [
    "actor_network = CustomActorNetwork(input_tensor_spec=obs_spec,\n",
    "                                   output_tensor_spec=act_spec,\n",
    "                                   fc_layer_params=fc_layer_params,\n",
    "                                   activation_fn=tf.keras.activations.relu,\n",
    "                                   activation_action_fn=tf.keras.activations.linear)\n",
    "critic_network = CriticNetwork(input_tensor_spec=(obs_spec, act_spec),\n",
    "                               observation_fc_layer_params=observation_fc_layer_params,\n",
    "                               action_fc_layer_params=action_fc_layer_params,\n",
    "                               joint_fc_layer_params=joint_fc_layer_params,\n",
    "                               activation_fn=tf.keras.activations.relu,\n",
    "                               output_activation_fn=tf.keras.activations.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVbb91AiUIdI"
   },
   "source": [
    "Criando o agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTaOktR1UPdw"
   },
   "outputs": [],
   "source": [
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "\n",
    "train_step = train_utils.create_train_step()\n",
    "\n",
    "agent = Td3AgentInvertingGradients(\n",
    "    time_step_spec=time_spec,\n",
    "    action_spec=act_spec,\n",
    "    actor_network=actor_network,\n",
    "    critic_network=critic_network,\n",
    "    actor_optimizer=actor_optimizer,\n",
    "    critic_optimizer=critic_optimizer,\n",
    "    target_update_tau=tau,\n",
    "    exploration_noise_std=exploration_noise_std,\n",
    "    exploration_noise_std_end=exploration_noise_std_end,\n",
    "    exploration_noise_num_steps=exploration_noise_num_steps,\n",
    "    target_policy_noise=target_policy_noise,\n",
    "    target_policy_noise_clip=target_policy_noise_clip,\n",
    "    actor_update_period=actor_update_period,\n",
    "    target_update_period=target_update_period,\n",
    "    train_step_counter=train_step,\n",
    "    gradient_clipping=gradient_clipping_norm,\n",
    "    gamma=discount)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW2zDV7cURNi"
   },
   "source": [
    "Replay Buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNBYocZnUTqe"
   },
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=agent.collect_data_spec,\n",
    "                                                               batch_size=tf_env_training.batch_size,\n",
    "                                                               max_length=buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVQKdOabUVSy"
   },
   "source": [
    "Criando o Driver e realizando coleta inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQVXZMW3UYo7"
   },
   "outputs": [],
   "source": [
    "# Data Collection (Collect for initial episodes)\n",
    "driver = dynamic_step_driver.DynamicStepDriver(env=tf_env_training,\n",
    "                                               policy=agent.collect_policy,\n",
    "                                               observers=[replay_buffer.add_batch],\n",
    "                                               num_steps=collect_steps_per_iteration)\n",
    "driver.run = common.function(driver.run)\n",
    "\n",
    "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(env=tf_env_training,\n",
    "                                                               policy=agent.collect_policy,\n",
    "                                                               observers=[replay_buffer.add_batch],\n",
    "                                                               num_steps=collect_steps_per_iteration)\n",
    "\n",
    "initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "\n",
    "for _ in range(initial_collect_episodes):\n",
    "    done = False\n",
    "    while not done:\n",
    "        time_step, _ = initial_collect_driver.run()\n",
    "        done = time_step.is_last()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Criando o dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating a dataset\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(64)\n",
    "\n",
    "iterator = iter(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Treinamento do Agente."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training\n",
    "agent.train = common.function(agent.train)\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    done = False\n",
    "    best_solution = np.finfo(np.float32).max\n",
    "    ep_rew = 0.0\n",
    "    while not done:\n",
    "        time_step, _ = driver.run()\n",
    "        experience, unused_info = next(iterator)\n",
    "        agent.train(experience)\n",
    "\n",
    "        # Acessando indíce 0 por conta da dimensão extra (batch)\n",
    "        obj_value = driver.env.get_info().objective_value[0]\n",
    "\n",
    "        if obj_value < best_solution:\n",
    "            best_solution = obj_value\n",
    "\n",
    "        ep_rew += time_step.reward\n",
    "        done = time_step.is_last()\n",
    "\n",
    "    print('episode = {0} Best solution on episode: {1} Return on episode: {2}'.format(ep, best_solution, ep_rew))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdyE8wpdUdW6"
   },
   "source": [
    "Realizando os testes com o agente treinado (Policy) para 1 único episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyqQoALeUfye"
   },
   "outputs": [],
   "source": [
    "evaluate_agent(tf_env_eval, agent.policy, function, dims, name_algorithm='TD3-IG', save_to_file=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldLYm4-NUfU_"
   },
   "source": [
    "Salvando policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CA51bmP_mNVy"
   },
   "outputs": [],
   "source": [
    "tf_policy_saver = PolicySaver(agent.policy)\n",
    "tf_policy_saver.save('policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqJzanqWUi3_"
   },
   "source": [
    "Fazendo download dos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfnjCK1DqgV4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from os import system\n",
    "agent_file_name = f'policy-td3-ig-{function.name}-{str(dims)}d.zip'\n",
    "system(f'zip -r {agent_file_name} policy/')\n",
    "files.download(agent_file_name)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TD3-IG-Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}